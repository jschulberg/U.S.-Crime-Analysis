---
title: "U.S. Crime Analysis"
author: "Justin Schulberg"
date: "5/22/2020"
output:
  word_document: default
  html_document: default
---

# U.S. Crime Analysis 

In this script, I will analyze the U.S. Crime dataset included in the data folder of this repository. I will use a variety of exploratory and modeling techniques to do so, including, but not limited to:  

  - Outlier Detection  
  - Linear Regression  
  - Principal Component Analysis (PCA) + Lin. Reg.  
  - Regression Trees  
  - Random Forest  
  - Variable Selection  
  
Let's start by taking a look at our data:

```{r set up, echo=FALSE}
#######################################################################
# Set Up --------------------------------------------------------------
#######################################################################
# Bring in packages
suppressMessages(library("dplyr")) # Used for data cleaning
suppressMessages(library("tidyr")) # Used for data cleaning
suppressMessages(library("ggplot2")) # Used for visualizations
suppressMessages(library("readxl")) # Used for loading excel files
suppressMessages(library("readr")) # Used for working with files
suppressMessages(library("factoextra")) # Used for PCA 
suppressMessages(library("here")) # Used for reading in files
suppressMessages(library("outliers")) # Used for outlier detection 
suppressMessages(library("rpart")) # Used for Decision Trees
suppressMessages(library("rpart.plot")) # Used for Decision Trees
suppressMessages(library("randomForest")) # Used for Random Forest
suppressMessages(library("glmnet")) # Used for various regression techniques
suppressMessages(library("caret")) # Used for general modeling, although I'm not a huge fan
suppressMessages(library("MASS")) # Used for variable selection

# Bring in the data, delimited by a tab ("\t")
data_crime <- read.delim(here::here("/Data/uscrime.txt"))

# Convert to a tibble, my preferred data structure
data_crime <- as_tibble(data_crime)

# Let's take a peek under the hood
head(data_crime)
summary(data_crime)
```


Below is a list of the variables in data along with their associated descriptions. We want to predict the last column, Crime, based on the other predictor variables.  
Variable	 	Description  
M		percentage of males aged 14-24 in total state population  
So		indicator variable for a southern state  
Ed		mean years of schooling of the population aged 25 years or over  
Po1		per capita expenditure on police protection in 1960  
Po2		per capita expenditure on police protection in 1959  
LF		labour force participation rate of civilian urban males in the age-group 14-24  
M.F		number of males per 100 females  
Pop		state population in 1960 in hundred thousands  
NW		percentage of nonwhites in the population  
U1		unemployment rate of urban males 14-24  
U2		unemployment rate of urban males 35-39  
Wealth		wealth: median value of transferable assets or family income  
Ineq		income inequality: percentage of families earning below half the median income  
Prob		probability of imprisonment: ratio of number of commitments to number of offenses  
Time		average time in months served by offenders in state prisons before their first release  
Crime		crime rate: number of offenses per 100,000 population in 1960  


# Outliers
In this section, we'll first test to see whether there are any outliers in the last column (number of crimes per 100,000 people). To do so, we'll use the grubbs.test function in the outliers package in R.

```{r boxplot, echo=FALSE}

# First, let's look at a boxplot of the data
ggplot(data_crime, aes(y = Crime)) +
  geom_boxplot(outlier.colour="slateblue4",
               outlier.size=2,
               color = "slateblue3") +
  theme_classic() +
  # Let's change the names of the axes and title
  labs(title = "Crime Rate for 47 States",
       subtitle = "Year = 1960",
       caption = "*Number of offenses per 100,000 population") +
  ylab("Crime Rate*") +
  # Center the title and format the subtitle/caption
  theme(plot.title = element_text(hjust = 0, color = "slateblue"),
        plot.subtitle = element_text(color = "slateblue1", size = 10),
        plot.caption = element_text(hjust = 1, face = "italic", color = "dark gray"),
        # remove the x axis labels because they don't mean much for us
        axis.text.x = element_blank()) +
  # I thought the boxplot was too thick, so let's make it a little skinnier
  scale_x_discrete(limits=c("-.1", ".1"))

```

Now that we have a clear idea of what the outliers could look like, let's use the grubbs.test to better understand these outliers.

```{r Grubbs, echo=FALSE}

data_grubbs <- data_crime 
(grubbs1 <- grubbs.test(data_grubbs$Crime))
grubbs1$alternative 
# This indicates that the highest value 1993 would be an outlier if there were one in the set

(p_value <- grubbs1$p.value) 

```

Since the p-value is above .05 (.079, to be exact), we cannot say with confidence that there is an outlier in the set. Let's move on to some modelling.

# Linear Regression  
In the first section, we'll use a simple linear regression model to predict on our crime data. Start by scaling the data so it's standardardized. Avoid column 2 because it's an indicator (factor) for southern states.

```{r lin reg, echo=FALSE}
data_scaled <- as_tibble(scale(data_crime[, c(1, 3:16)]))

# bring column 2 back in and reorder our columns
data_scaled <- data_scaled %>%
  cbind(data_crime[, 2]) %>%
  dplyr::select(Crime, everything())

# Before we build our first Linear Regression model, let's set our seed to ensure randomness
set.seed(123)

# Let's also turn off scientific notation, because it'll make our results messy to read
options(scipen = 999)

### Build our first model using every variable as an output. Print the results
(model_lm1 <- lm(Crime ~ ., data = data_scaled))

summary(model_lm1)
```

The first two items we should look at are the R-squared value (80.3%) and the Adjusted R-squared value (71.8%). The next item we should look at is the overal p-value, which is p-value: 0.0000003539. This means that our crime data can be explained by the amalgam of predictor variables we provided.


```{r lin reg coefs, echo=FALSE}
# What do our coefficients look like?
model_lm1$coefficients
```

As an example of what this means, if M (percentage of males aged 14-24 in total state population) increases by 1, we would expect crime to increase by 87.83 crimes per 100,000 population.  

But which of the variables mattered and which did not? Well, to start, let's look at the individual p-values for each variable we have. We'll do this by pulling out the p-values and coefficient names.

```{r variable selection, echo=FALSE}
p_vals <- summary(model_lm1)$coefficients[,4]
coefs <- rownames(summary(model_lm1)$coefficients)

# bring our coefficient names and p-values into a data frame
p_vals_df <- as_tibble(cbind(coefs, p_vals))

# remove the first row which includes the intercept
p_vals_df <- p_vals_df[-1, ]

# Make sure the p-values are read in as numerics
p_vals_df$p_vals <- as.double(p_vals_df$p_vals)

# Visualization time
ggplot(p_vals_df,
       # order by size of the p-values
       aes(x = reorder(coefs, -p_vals), y = p_vals)) +
  # Let's make it a bar graph and change the color
  geom_bar(stat = "identity", fill = "slateblue2") +
  # Change the theme to minimal
  theme_classic() +
  # Let's change the names of the axes and title
  xlab("Predictor Variable") +
  ylab("P-Value*") +
  labs(title = "P-Values for Different Predictors in the U.S. Crime Dataset",
       subtitle = "*The dashed line shows which p-values are above or below the threshold of .05") +
  # format our title and subtitle
  theme(plot.title = element_text(hjust = 0, color = "black"), 
        plot.subtitle = element_text(color = "dark gray", size = 10)) +
  # let's create a gray dashed, drop-line at .05 so we can see which variables are under our threshold
  geom_hline(yintercept = .05, linetype = "dashed", color = "light gray", size = .75) +
  # flip the axes and fix the axis
  coord_flip(ylim = c(0, 1)) 

```

Let's confirm what we see above, especially since U2 seems on the border. Filter our data frame to only show p-values less than .05 and then order our results.

```{r significant variables, echo=FALSE}
signif_vars <- p_vals_df %>%
  filter(p_vals < 0.05) %>%
  arrange(p_vals)

# Pull the data back in
data_signif <- data_scaled[which(colnames(data_scaled) %in% signif_vars$coefs)]

# Bring Crime column back in
data_signif <- cbind(data_signif, Crime = data_scaled$Crime)
```

Now that we have the coefficients that matter, let's re-run the linear model only on the coefficients we see above

```{r second lin reg, echo=FALSE}
model_lm2 <- lm(Crime ~ ., data_signif)

summary(model_lm2)
```

We see from this model that our R-squared value dropped significantly. Very strange, and not what I expected. Let's try bringing in the two variables that were just above the .05 threshold (U@ and Po1).

```{r third lin reg, echo=FALSE}
data_updated <- cbind(data_signif, Po1 = data_scaled$Po1)

model_lm3 <- lm(Crime ~., data_updated)

summary(model_lm3)
```

Great! Our R-squared value went up to 76.6% and our adjusted R-squared went up to 73.1%. In particular, we notice that our R-squared value has gone down (it was originally 80.3%); however, our adjusted R-squared value has gone up (it was originally 70.1%). This makes sense because adjusted R-squared factors in the number of variables, and should get closer to 1 the less we overfit our model.

Now, let's predict on a fake dataset given by the original prompt to see what we get.

```{r lin reg 3, echo=FALSE}

# Let's check out our coefficients with the new model
model_lm3$coefficients

# Now, let's predict on this fake dataset
M = 14.0
So = 0
Ed = 10.0
Po1 = 12.0
Po2 = 15.5
LF = 0.640
M.F = 94.0
Pop = 150
NW = 1.1
U1 = 0.120
U2 = 3.6
Wealth = 3200
Ineq = 20.1
Prob = 0.04
Time = 39.0

# Bring all this new data into one data frame, transposed so it's all in one row and counts as one observation
new_data <- as.data.frame(t(c(M = M, So = So, Ed = Ed, Po1 = Po1, Po2 = Po2, 
                              LF = LF, M.F = M.F, Pop = Pop, NW = NW, U1 = U1,
                              U2 = U2, Wealth = Wealth, Ineq = Ineq, Prob = Prob, Time = Time)))


# Predict what the crime rate would now be
predict(model_lm3, new_data) 
```

The linear regression model estimates that the Crime rate based on this data will be 35.79. This translates to about 36 crimes per 100,000 population. This seems extremely off from our original dataset, so we'll instead resort to our original, unscaled data which included every variable.

Now let's take a look at the confidence interval for this new data.

```{r, lin reg predictions, echo=FALSE}
predict(model_lm3, new_data, interval = "confidence")
```

The same linear regression model estimates 798 crimes per 100,000 population. It also estimates that the lower limit for our data is -886.6 and the upper limit is 2482.3. This result makes more sense based on the fact that the median of our original crime is **`r median(data_crime$Crime)`**.


# Principal Component Analysis  

In this section, I'll apply Principal Component Analysis, a method for trimming down the number of variables in our dataset and for variable selection. This *should* help with model-building later on.

```{r pca prep, echo=FALSE}
# Start by getting rid of the Crime column for now since we won't perform
# PCA on the one column we are aiming to predict.
base_data <- data_crime %>%
  dplyr::select(-Crime)

head(base_data)

# Because we'll need this to unscale our data later on, let's track the means and standard deviations
# of each variable
mu <- sapply(base_data, mean)
std_devs <- sapply(base_data, sd)

# Before we continue, let's set our seed to ensure randomness
set.seed(123)

# Let's also turn off scientific notation, because it'll make our results messy to read
options(scipen = 999)

```

Run our first Principle Component Analysis using the prcomp() function which uses Singular value decomposition (SVD), which examines the covariances / correlations between individual observations.

```{r pca1, echo=FALSE}
# Note: We won't scale the data within the function because we already scaled it earlier.
(pca1 <- prcomp(base_data, scale. = T))

summary(pca1)
```

We can see that, from our Cumulative Proportion dimension, the first six principle components explain about 90% of the variation in the data.

```{r pca plot1, echo=FALSE}

(ind_plot <- fviz_pca_ind(pca1,
                          # Shade the viz by the quality of representation
                          col.ind = "slateblue2", 
                          # Avoid text overlapping
                          repel = T,
                          # Add some transparency so it's easier to see points overlayed on top of each other
                          alpha.ind = .5
)
)
```

Above is a graph of the individual observations. Individual states with a similar profile of factors will be grouped. Next, we'll create a graph of the variables (columns in our dataset).  
 - Positively correlated variables point to the same side of the plot.  
 - Negatively correlated variables point to opposite sides of the plot.  
 
```{r pca plot2, echo=FALSE}
fviz_pca_var(pca1,
             # Shade the viz by the quality of representation
             col.var = "slateblue2", 
             # Avoid text overlapping
             repel = T
)
```


What are our eigenvalues?
```{r pca eigen, echo=FALSE}
(pca_eig_val <- get_eigenvalue(pca1))
```
Note that the variance.percent values noted here align with our screeplot above. This also now tells us that 90% of the variance in our data is explained by just 6 dimensions. Let's graph the cumulative variance percentages so it's easier to visualize.

```{r pca plot3, echo=FALSE}
# Before we do so, let's bring the dimension numbers in as a column.
pca_cum_var <- pca_eig_val %>%
  mutate("dimension" = row.names(pca_eig_val))

# Clean up the dimensions so they're more interpretable and make sure they're read in as factors
pca_cum_var$dimension <- as.factor(substr(pca_cum_var$dimension, 5, length(pca_cum_var$dimension)))

# Viz time
ggplot(pca_cum_var, aes(x = dimension)) +
  # Create our bars
  geom_col(aes(y = variance.percent), fill = "slateblue1") +
  # Let's try to add a line to represent our cumulative variance to close out our visualization
  geom_path(aes(y = cumulative.variance.percent, group = 1), color = "slateblue1", size = .5) +
  # Let's try to overlay the cumulative variance as points
  geom_point(aes(y = cumulative.variance.percent),  color = "slateblue4", size = 2) +
  # Ensure that everything's ordered by dimension
  scale_x_discrete(limits = pca_cum_var$dimension) +
  theme_classic() +
  # Let's change the names of the axes and title
  labs(title = "Variation explained by the dimensions in our dataset",
       subtitle = "This uses the method of Principle Component Analysis",
       y = "Variance (%)",
       x = "Dimension") +
  # Center the title and format the subtitle/caption
  theme(plot.title = element_text(hjust = 0, color = "slateblue4", size = 14),
        plot.subtitle = element_text(color = "dark gray", size = 10))

```



Just like earlier, we'll bring our crime data (our dependent variable) back in and see how our predictions are using the principle components. 

```{r pca lin reg, echo=FALSE}
# First, let's limit our data to just the first six principle components, based on
# our analysis above.
# Create a variable for the number of coefficients we'll be pulling
pca_len <- 6
pca_vals <- pca1$x[, 1:pca_len]

pca_vals <- pca_vals %>%
  cbind(Crime = data_crime$Crime) %>%
  as_tibble()

mod1 <- lm(Crime ~ ., data = pca_vals)

summary(mod1)
```

Notice that our adjusted R-squared value is 60.7%. This is significantly lower than what we saw in the previous homework (somewhere around 75-80%), but we've significantly reduced the number of variables we're using.

With that in hand, we'll transform our data back, since PCA performs a linear transformation to our dataset.
```{r retransform data, echo=FALSE}
### Transform our data back
# Get the original intercept from our model
b0 <- mod1$coefficients[1]

# Pull out model coefficients so we can make the Beta vector
b_vals <- mod1$coefficients[2:(pca_len + 1)]

# To get our data re-transformed, we'll have to multiply the coefficients by our rotated matrix, A to create alpha vector
# Note: The "%*%" operator enables us to perform matrix multiplication.
# First let's verify that we are using the correctly-sized objects for this.
if ( dim(pca1$rotation[, 1:pca_len])[2] == length(b_vals) ) {
  "Great work. Let's keep on going."
} else {
  "Hold on. Something seems wrong."
}

rota_matr <- pca1$rotation[, 1:pca_len] %*% b_vals

# Using the standard deviation values (std_devs) and mean values (mu) from our original dataset, 
# let's recover (unscale) our original coefficient values by dividing the rotation matrix vector by our standard deviations
# and get our original intercept by subtracting the sum of (rota_matr * mu)/std_devs from the intercept
orig_coefs <- rota_matr / std_devs
orig_b0 <- b0 - sum(rota_matr * mu / std_devs)

## Get our original data's formula (coefficients + intercept) such that our model is of the form Y = aX + b
# Before we get the original formula, let's verify that we are using the correctly-sized objects for this. This is important
# for matrix multiplication
# if ( dim(data[, 1:15])[2] == dim(orig_coefs)[1] ) {
#   "Great work. Let's keep on going."
# } else {
#   "Hold on. Something seems wrong."
# }

estimates <- as.matrix(data_crime[, 1:15]) %*% orig_coefs + orig_b0

summary(estimates)
```

The estimates represent what the PCA model expects our crime data to be based on the six principle components we identified. These are not exact, because our PCA model really only explains about 90% of the variation within our dataset.

As we did earlier, now, let's predict on the dataset we've been provided. First we'll store all the new data we've been provided into a data frame.

```{r, echo=FALSE}
M = 14.0
So = 0
Ed = 10.0
Po1 = 12.0
Po2 = 15.5
LF = 0.640
M.F = 94.0
Pop = 150
NW = 1.1
U1 = 0.120
U2 = 3.6
Wealth = 3200
Ineq = 20.1
Prob = 0.04
Time = 39.0

# Bring all this new data into one data frame so it's all in one row and counts as one observation.
new_data <- data.frame(M = M, So = So, Ed = Ed, Po1 = Po1, Po2 = Po2, 
                       LF = LF, M.F = M.F, Pop = Pop, NW = NW, U1 = U1,
                       U2 = U2, Wealth = Wealth, Ineq = Ineq, Prob = Prob, Time = Time)


# Convert the new state data using the PCA we found earlier so we can apply our model.
(new_pca <- data.frame(predict(pca1, new_data)))
```

So where does this new "state" fit? We'll use the graph we saw earlier and add this new point, manipulated utilizing our PCA, as a red dot on the graph.

```{r pca plot4, echo=FALSE}
fviz_add(ind_plot, new_pca, color ="red")
```

Now we'll predict what the crime rate would now be by leveraging the new data that's had PCA applied.

```{r pca predict, echo=FALSE}
(pred <- predict(mod1, newdata = new_pca))
```

Thus we predict that the crime rate in the new state would be 1248 crimes per 100,000 population. This seems within the range of reason for our data, especially given that the mean of our crime data is **`r mean(data_crime$Crime)`** and the range goes from **`r range(data_crime$Crime)[1]`** to **`r range(data_crime$Crime)[2]`**.  


# Decision Trees
In this section, I'll find the best model available using:
  - Regression Trees
  - Random Forest
  
```{r decision trees, echo=FALSE}
# Change column 2 to be a factor
data_scaled_facs <- as.data.frame(as.factor(unlist(data_crime[, 2])))

# bring column 2 back in and reorder our columns
data_scaled_fix <- data_scaled %>%
  cbind(data_scaled_facs) %>%
  dplyr::select(Crime, everything()) %>%
  rename("S0" = "as.factor(unlist(data_crime[, 2]))") %>%
  as_tibble()
```

## Regression Tree  
Recursive partitioning is a fundamental tool in data mining. It helps us explore the stucture of a set of data, while developing easy to visualize decision rules for predicting a categorical (classification tree) or continuous (regression tree) outcome. Our formula	below will be in the format outcome ~ ., which allows us to predict on all of our variables.  

We'll grow our tree using the rpart function from the rpart package to create our regression tree.

```{r rpart, echo=FALSE}
reg_tree <- rpart(Crime ~ ., 
                  data = data_scaled_fix, # specifies the data frame (we'll use our scaled data so everything's on equal footing)  
                  method = "anova") # "anova" for a regression tree*
# control=	optional parameters for controlling tree growth. For example, 
# control = rpart.control(minsplit=30, cp=0.001) requires that the minimum number 
# of observations in a node be 30 before attempting a split and that a split must 
# decrease the overall lack of reg_tree by a factor of 0.001 (cost complexity factor) 
# before being attempted.
summary(reg_tree)
```

Immediately, we see that our four most important variables in this tree model are:  
 1. Po1 (per capita expenditure on police protection in 1960)  
 2. Po2 (per capita expenditure on police protection in 1959)  
 3. Wealth (median value of transferable assets or family income)  
 4. Ineq (percentage of families earning below half the median income)  
 
```{r decision tree analysis, echo=FALSE}
 # display complexity parameter table
(cp <- reg_tree$cptable)
plotcp(reg_tree)	

```


Notice from the graph of the relative errors, that our best split occurs when cp = 4, which is where the lowest cross valiadation error occurs. This means that the optimal number of splits for our tree is 4. This value occurs at cp = **`r cp[which.min(cp[, 3]), 1]`**.

```{r, echo=FALSE}
# plot approximate R-squared and relative error for different splits (2 plots).
rsq.rpart(reg_tree)	

prp(reg_tree)
```

From the visualization of the regression tree above, we can see that Po1 is the most significant variable (which aligns with the output of our rpart() function), and then Pop and NW are the next most important. After that, not many of the variables are that important for our purposes.  


## Random Forest  
Now we'll compare our results to those we get from running a Random Forest model. The Random Forest model will make 500 decision trees and "vote" on the nodes that work best (i.e. provide the highest accuracy).
```{r random forest, echo=FALSE}
rf_mod <- randomForest(Crime ~ ., data = data_scaled_fix)

print(rf_mod)
```

Here we note that the mean of the squared residuals is **`r last(rf_mod$mse)`**. The Random Forest algorithm we ran explains about 56.7% of the variation in our data, which is not great.

Which variables were the most important according to the Random Forest algorithm?

```{r, echo=FALSE}
# Start by bringing everything together into one data frame and rename the columns
rf_imprtnce <- cbind(row.names(rf_mod$importance), rf_mod$importance)
colnames(rf_imprtnce) <- c("Variable", "Importance")
rf_imprtnce <- as_tibble(rf_imprtnce)

# make the importance variable a numeric and round it
rf_imprtnce$Importance <- as.numeric(rf_imprtnce$Importance)
rf_imprtnce$Importance <- round(rf_imprtnce$Importance, 2)

# Visualization time
ggplot(rf_imprtnce,
       # order by importance
       aes(x = reorder(Variable, Importance), y = Importance, group = 1)) +
  # Let's make it a column graph and change the color
  geom_col(fill = "slateblue2") +
  # Add the text labels in for p-values so it's easier to read
  geom_label(label = rf_imprtnce$Importance, size = 3, hjust = 0, color = "black") +
  # Change the theme to classic
  theme_classic() +
  # Let's change the names of the axes and title
  xlab("Variables") +
  ylab("Importance") +
  labs(title = "Importance of Different Variables\nin Random Forest Algorithm",
       subtitle = "The dataset used is the Crime dataset") +
  # format our title and subtitle
  theme(plot.title = element_text(hjust = 0, color = "black"),
        plot.subtitle = element_text(color = "dark gray", size = 10)) +
  # flip the axes and fix the axis
  coord_flip()
```

It's interesting to note here that the three most important variables for the Random Forest algorithm are similar to the three we found in the Regression Tree earlier:  
1. Po1 (per capita expenditure on police protection in 1960)  
2. Po2 (per capita expenditure on police protection in 1959)  
3. Wealth (median value of transferable assets or family income)  


# Variable Selection  
In this section, I'll attempt multiple methods for narrowing down our dataset by selecting the most important variables. As I narrow down the dataset, I'll compare different linear regression models to
see if our accuracy improves.  

## Stepwise Regression   
First we'll find the full linear regression model and then run Stepwise Regression, in which we'll slowly pick apart the full model by slicing out negligible coefficients.

```{r, echo=FALSE}
# Using the linear regression model we created earlier, fit a stepwise regression model
step_mod <- stepAIC(model_lm1, direction = "both", trace = FALSE)

summary(step_mod)
```

This puts our R-squared value around 74% and p-value close to 0

Next we'll use the caret package -- specifically the leaps and the MASS packages to fit our linear regression model using stepwise selection (leapSeq). Set up repeated 10-fold cross-validation, which will let us estimate the RMSE (average prediction error) for each of the 5 models specified by nvmax.

```{r stepwise, echo=FALSE}
train_control <- trainControl(method = "cv", number = 10)
# Train the model
step_mod <- train(Crime ~ ., data = data_crime,
                    method = "leapSeq", # stepwise selection
                    tuneGrid = data.frame(nvmax = 1:5), # maximum number of predictors to be incorporated in the model. This will search the best 5-variable models for us
                    trControl = train_control
                    )

step_mod$results
# The output above shows different metrics and their standard deviation for 
# comparing the accuracy of the 5 best models.
cat("We can see that an nvmax of", step_mod$bestTune[[1]], "is the best model with an RMSE of", step_mod$results$RMSE[[which.min(step_mod$results$RMSE)]])

# save our stepwise regression model results in a dataframe
step_res <- step_mod$results
step_res$RMSE <- round(step_res$RMSE, 3)

# Visualization time
ggplot(step_res,
       # order by importance
       aes(x = reorder(nvmax, -RMSE), y = RMSE, group = 1)) +
  # Let's make it a column graph and change the color
geom_col(fill = "slateblue2") +
  # Add the text labels in for p-values so it's easier to read
  geom_text(label = step_res$RMSE, size = 5, hjust = 1.1, color = "white") +
  # Change the theme to classic
  theme_classic() +
  # Let's change the names of the axes and title
  xlab("Number of Predictors to Include") +
  ylab("Average Mean Squared Error") +
  labs(title = "Average Mean Squared Error for Different Predictors",
       subtitle = "This analysis uses Stepwise Regression by\nleveraging the leaps and MASS packages") +
  # format our title and subtitle
  theme(plot.title = element_text(hjust = 0, color = "black"),
        plot.subtitle = element_text(color = "dark gray", size = 10)) +
  # flip the axes and fix the axis
  coord_flip()

```


Our final model and coefficients are:
```{r stepwise results, echo=FALSE}
summary(step_mod$finalModel)
coef(step_mod$finalModel, 3)
```

An asterisk indicates that a given variable is going to be included in our final model. For example, since we got the lowest RMSE of **`r step_mod$results$RMSE[[which.min(step_mod$results$RMSE)]]`** value with **`r step_mod$results$nvmax[[which.min(step_mod$results$RMSE)]]`** predictors, we are going to include:  
  
 1. Ed (mean years of schooling of the population aged 25 years or over)  
 2. Po1 (per capita expenditure on police protection in 1960)  
 3. Ineq (income inequality: percentage of families earning below half the median income)  

Thus our final model becomes:  
```{r final stepwise, echo=FALSE}
cat("Crime = ", coef(step_mod$finalModel, 3)[1], "+ (Ed x ", coef(step_mod$finalModel, 3)[2], ") + (Po1 x ", coef(step_mod$finalModel, 3)[3], ") + (Ineq x ", coef(step_mod$finalModel, 3)[4], ")", sep = "")
```
  
  
## Lasso and Elastic-Net Regression   
After doing some research, I found that both variable selection methods use the same glmnet() functions. The only difference is an input parameter, denoted 'alpha', which differs between the two. So we'll just look at them simultaneously, along with a number of other variations of alpha values.

```{r lasso, echo=FALSE}
# Super annoying but glmnet prefers data.matrix instead of as.matrix
x <- data.matrix(data_scaled_fix[, -16])
y <- data.matrix(data_scaled_fix[, 16])

# Let's loop through a bunch of alpha values from 0 to 1, sequenced on a .1 basis
a_vals <- seq(0, 1, .1)

# Initialize an empty list to hold all of our models
alphafit_i <- list()

# Initialize an empty dataframe to hold all of our final values
full_df <- data.frame()

# Build the lasso regression and test our outputs for all the alpha values from .1 to 1
for (i in a_vals) {
  # Next we'll run the cv.glmnet function on our different alpha values
  alphafit_i <- cv.glmnet(x, y, type.measure = "mse", alpha = i, nfolds = 5, family = "gaussian")
  # Find each alpha value's lambda.1se, which is the largest value of lambda such that error is within 1 standard error of the        minimum.
  lam_i <- alphafit_i$lambda.1se
  # Run our predictions. Note that s probably refers to the size of the penalty we are setting
  prediction <- predict(alphafit_i, s = lam_i, newx = x)
  # Get our mean squared error
  mse <- mean((y - prediction)^2)
  # Create a temporary, new data frame
  temp_df <- data.frame(a_vals = i, mse = mse)
  # Recursively build our data frame
  full_df <- rbind(full_df, temp_df)
}

# round our mean squared errors
full_df$mse <- round(full_df$mse, 5)

# Visualization time
ggplot(full_df,
       # order by mean squared error
       aes(x = reorder(a_vals, -mse), y = mse, group = 1)) +
  # Let's make it a column graph and change the color
  geom_col(fill = "slateblue2") +
  # Add the text labels in for mse-values so it's easier to read
  geom_label(label = full_df$mse, size = 3, hjust = 0, color = "black") +
  # Change the theme to classic
  theme_classic() +
  # Let's change the names of the axes and title
  xlab("Alpha Values") +
  ylab("Mean Squared Error") +
  labs(title = "Mean Squared Error of Different Alpha Values",
       subtitle = "Alpha = 0 means Ridge Regression\nAlpha = .5 means Elastic-Net\nAlpha = 1 means Lasso Regression") +
  # format our title and subtitle
  theme(plot.title = element_text(hjust = 0, color = "black"),
        plot.subtitle = element_text(color = "dark gray", size = 10)) +
  # flip the axes and fix the axis
  coord_flip()

cat("From above, we can see that an alpha value of ", full_df$a_vals[which.min(full_df$mse)], " produces our lowest Mean Squared Error of ", full_df$mse[which.min(full_df$mse)], ".", sep = "")  
```

### Find our Model   
Now that we know which alpha value gives us the lowest Mean Squared Error, we'll pull that model out and treat that as our linear regression model.

```{r elastic net, echo=FALSE}
# Store our min alpha value as a variable
min_alpha <- full_df$a_vals[which.min(full_df$mse)]
# Next we'll run the cv.glmnet function on our optimal alpha value of .1
alphafit_optimal <- cv.glmnet(x, y, type.measure = "mse", alpha = min_alpha, nfolds = 5, family = "gaussian")
```

Find the alpha value's lambda.1se, which is the largest value of lambda such that error is within 1 standard error of the minimum.
```{r lambda, echo=FALSE}
(lambda <- alphafit_optimal$lambda.1se)
```

Run our prediction on our model. Note that s probably refers to the size of the penalty we are setting.
```{r var select predict, echo=FALSE}
(prediction <- predict(alphafit_optimal, s = lambda, newx = x))
```

Get our mean squared error
```{r mse, echo=FALSE}
(mse <- mean((y - prediction)^2))
```

Now that we have our glmnet conducted, we'll figure out which coefficients we need so we can build out our linear regression model.

```{r var selection, echo=FALSE}
# Find the coefficients that are most important
coefs <- coef(alphafit_optimal, s = lambda)

# With these coefficients, we'll rebuild our linear model.

# Reorder our crime data to match the correct order of columns
reordered_crime <- data_crime %>%
  dplyr::select(So, everything())

# Pull out the variables of interest
col_of_interest <- as.vector(summary(coefs)[i]-1)

# Note that the first variable in this vector is the intercept, so let's remove that
col_of_interest1 <- col_of_interest[-1, ]

# build our data frame with the proper variables
new_crime <- reordered_crime[, col_of_interest1]

# Bring the crime variable back in
new_crime <- new_crime %>%
  cbind(Crime = data_crime$Crime)

# run the linear regression model
final_model <- lm(Crime ~ ., data = new_crime)

summary(final_model)
```

We can see that our Adjusted R-squared value is 66%. Not all of the coefficients seem to have low p-values, and are thus not significant, so I would suggest taking more of a manual approach to sifting through the data and selecting variables.

### Thanks for reading!


